quarkus.langchain4j.ollama.timeout = 3m

quarkus.langchain4j.ollama.chat-model.model-id = codellama
quarkus.langchain4j.ollama.chat-model.format = json

# TODO: If we remove this line, the default temperature seems to 0.0 in JVM mode and 0.8 in native mode
quarkus.langchain4j.ollama.chat-model.temperature = 0

# Uncomment to log HTTP traffic between langchain4j & the LLM API
quarkus.rest-client.logging.scope=request-response
quarkus.rest-client.logging.body-limit=10000
quarkus.log.category."org.jboss.resteasy.reactive.client.logging".level=DEBUG