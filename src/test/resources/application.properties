quarkus.langchain4j.ollama.chat-model.model-id=codellama
quarkus.langchain4j.ollama.log-requests=false
quarkus.langchain4j.ollama.log-responses=false
quarkus.langchain4j.ollama.chat-model.temperature=0.0
#quarkus.langchain4j.ollama.chat-model.top-p=0.0
#quarkus.langchain4j.ollama.chat-model.top-k=0
quarkus.langchain4j.ollama.chat-model.format=json
quarkus.langchain4j.ollama.timeout=3m
